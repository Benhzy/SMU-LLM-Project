{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Testing the Prediction Model Pipeline\n","This notebook tests the `ModelPredictor` class which uses predefined system prompts to generate answers for different scenarios. Below are the steps to use this notebook:"]},{"cell_type":"markdown","metadata":{},"source":["## Setup\n","First, make sure that all required libraries are installed and import the necessary modules. You will also need to ensure that the prediction model and system prompts scripts are available as modules."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import json\n","from prediction_model import ModelPredictor\n","from system_prompts import SystemPrompts\n","from datetime import datetime\n","prompts_manager = SystemPrompts('system_prompts.json')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Load Data\n","Load the data which contains various scenarios and questions that will be processed by the prediction model."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["with open('../data/processed/extracted_data.json', 'r') as file:\n","    data = json.load(file)"]},{"cell_type":"markdown","metadata":{},"source":["## Load System Prompt"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Prompt not found\n"]}],"source":["# Retrieve and print a system prompt using the SystemPrompts class\n","prompt_key = 'ans_tort_qns'\n","sys_prompt = prompts_manager.retrieve(prompt_key)\n","print(sys_prompt)"]},{"cell_type":"markdown","metadata":{},"source":["## Generate Answers\n","This function will use the `ModelPredictor` to generate answers for the loaded questions multiple times, based on the epochs specified."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_answers(data, sys_prompt, epoch):\n","    predictor = ModelPredictor(\n","        system_prompt=sys_prompt,\n","    )\n","\n","    # Iterate over each file in the JSON data\n","    for item in data:\n","        scenario = item['scenario']\n","        questions = item['questions']\n","        answers = {}\n","\n","        # For each question, prepare the prompt and get the model's prediction multiple times\n","        for i, question in enumerate(questions):\n","            question_key = f\"question_{i + 1}\"\n","            answers[question_key] = []\n","\n","            for _ in range(epoch):\n","                prompt = f\"Scenario: {scenario}\\nQuestion: {question}\\n\\nMy answer is:\"\n","                answer = predictor.predict(prompt)\n","                print(answer)\n","                answers[question_key].append(answer)\n","\n","        # Add the answers to the data\n","        item['answers'] = answers\n","\n","    return data"]},{"cell_type":"markdown","metadata":{},"source":["## Run Prediction\n","Execute the prediction process and save the updated data with the generated answers."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["updated_data = generate_answers(data, sys_prompt, epoch=3)\n","\n","current_datetime = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","output_filename = f'results/exam_answers_{current_datetime}.json'\n","with open(output_filename, 'w') as file:\n","    json.dump(updated_data, file, indent=4)\n","\n","print(f\"Processing complete. The updated data is saved in '{output_filename}'.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Store a new prompt using the SystemPrompts class\n","new_prompt_text = \"\"\"\n","Your new prompt text goes here.\n","\"\"\"\n","prompts_manager.store(prompt_key, new_prompt_text)\n","print('New prompt stored successfully.')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
